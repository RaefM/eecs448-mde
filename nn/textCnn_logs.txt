[maroofr@gl1023 ~]$ source env/bin/activate
(env) [maroofr@gl1023 ~]$ cat model.py
cat: model.py: No such file or directory
(env) [maroofr@gl1023 ~]$ cd eecs448-mde/nn
(env) [maroofr@gl1023 nn]$ cat model.py
import time
import copy
from typing import List, Dict

from nltk.tokenize import word_tokenize, sent_tokenize
from tqdm import tqdm
from gensim.models import KeyedVectors
import torch
from torch.utils.data import Dataset
from torch import float32, nn
import torch.nn.functional as F
import torch.optim as optimizer
import matplotlib.pyplot as plt
import gensim
import numpy as np
from sklearn.metrics import balanced_accuracy_score, accuracy_score

def tensor_embeds(embed):
    return {k: torch.FloatTensor(embed[k]) for k in embed.index_to_key}

def get_word_embedding(embed: Dict[str, torch.Tensor], unk_rep: torch.Tensor, word: str) -> torch.Tensor:
    return embed[word] if word in embed else unk_rep

def get_paragraph_embedding(embed: Dict[str, torch.Tensor], unk_rep: torch.Tensor, words: List[str]) -> torch.Tensor:
    return torch.vstack([
        get_word_embedding(embed, unk_rep, word) for word in words
    ])

class AITADataset(Dataset):
    def __init__(self, posts, labels, embed: Dict[str, torch.Tensor], unk: torch.Tensor):
        super().__init__()
        self.embed = embed
        self.unk = unk
        self.posts = [word_tokenize(post.lower()) for post in posts]
        self.labels = labels
    
    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
          'post': get_paragraph_embedding(self.embed, self.unk, self.posts[idx]),
          'label': self.labels[idx]
        }


def basic_collate_fn(batch):
    """Collate function for basic setting."""
    posts = [i['post'] for i in batch]
    labels = torch.IntTensor([i['label'] for i in batch])

    return posts, labels

#######################################################################
################################ Models ###############################
#######################################################################

class BiGRU(nn.Module):
    def __init__(
        self,
        rnn_dense_hidden_dim: int,
        device: str,
        dropout_rate: float = 0.25,
        word_vec_length: int = 300
    ):
        super().__init__()
        self.device = device
        self.bigru = nn.GRU(
            word_vec_length,     # input is each word embedding
            word_vec_length,     # hidden representation is same size as word embeddings
            batch_first=False,
            bidirectional=True
        )
        self.bigruDropout1 = nn.Dropout(dropout_rate)
        self.bigruDense = nn.Linear(word_vec_length * 2, rnn_dense_hidden_dim)
        self.bigruDropout2 = nn.Dropout(dropout_rate)
        self.bigruOutput = nn.Linear(rnn_dense_hidden_dim, 1)
        
    def forward(self, posts):
        #### BiGRU recurrent layer ####################################################
        # posts shape: batch length * num_words_per_post (ragged) * w2vlen
        input_lengths = [seq.size(0) for seq in posts]
        padded_input = nn.utils.rnn.pad_sequence(posts) # tensor shape (max_post_len, batch_len, w2vlen)
        total_length = padded_input.size(0)
        packed_input = nn.utils.rnn.pack_padded_sequence(
            padded_input, input_lengths, batch_first=False, enforce_sorted=False
        )
        packed_output, _ = self.bigru(packed_input) # shape (max_post_len, batch_len, rnn_hidden_dim)
        output, _ = nn.utils.rnn.pad_packed_sequence(
            packed_output, batch_first=False, total_length=total_length
        )
        # compute max pooling along the time dimension to collapse into a single rnn_hidden_dim vector
        rnn_embeddings = torch.max(output, dim=0).values
        rnn_embeddings = self.bigruDropout1(rnn_embeddings)
        #### BiGRU hidden layer #######################################################
        rnnDenseOut = torch.tanh(self.bigruDense(rnn_embeddings))
        rnnDenseOut = self.bigruDropout2(rnnDenseOut)
        ### BiGRU output layer ########################################################
        return self.bigruOutput(rnnDenseOut).flatten()
    
class textCNN(nn.Module):
    def __init__(
        self, 
        cnn_dense_hidden_dim: int,
        device: str,
        dropout_rate: float = 0.25,
        num_filters: int = 100,
        kernel_sizes: List[int] = [3, 4, 5],
        word_vec_length: int = 300
    ):
        super().__init__()
        # transpose input to get N * w2vlen * L, then  ===>  N * (num_filters x w2vlen) * L'
        self.convs = nn.ModuleList(
            [nn.Conv1d(word_vec_length, num_filters, k) for k in kernel_sizes]
        )
        # Compute max pooling along the L axis (not shown here), yielding N * (num_filters x w2vlen)
        self.cnnDropout1 = nn.Dropout(dropout_rate)
        self.cnnDense = nn.Linear(num_filters * len(kernel_sizes), cnn_dense_hidden_dim)
        self.cnnDropout2 = nn.Dropout(dropout_rate)
        self.cnnOutput = nn.Linear(cnn_dense_hidden_dim, 1)
        
    def forward(self, posts):
         #### Input reshaping ###########################################################
            # N * num_words_per_seq (ragged) * w2vlen ==> N * max_seq_len * w2vlen
        padded_input = nn.utils.rnn.pad_sequence(posts, batch_first=True) 
            # N * max_seq_len * w2vlen ==> N * w2vlen * max_seq_len (treat word2vec dimensions as channels)
        channelled_input = torch.transpose(padded_input, 1, 2)
        #### CNN clf convolutional layer ###############################################
            # Convolution: N * w2vlen * max_seq_len ==> N * num_filters * (max_seq_len - 2)
            # Pooling: N * num_filters * (max_seq_len - 2) ==> N * num_filters
        convOuts = [F.relu(conv(channelled_input)) for conv in self.convs]
        pooledOuts = [torch.max(convOut, dim=2).values for convOut in convOuts]
        pooledOut = torch.cat(pooledOuts, 1)
        pooledOut = self.cnnDropout1(pooledOut)
        #### CNN hidden layer ##########################################################
        cnnDenseOut = F.relu(self.cnnDense(pooledOut))
        cnnDenseOut = self.cnnDropout2(cnnDenseOut)
        #### CNN output layer ##########################################################
        return self.cnnOutput(cnnDenseOut).flatten()
        
class ensembleCNNBiGRU(nn.Module):
    """
    Based on:
    https://arxiv.org/pdf/1805.01890.pdf RMDL
    With code modelled off of:
    https://towardsdatascience.com/deep-learning-techniques-for-text-classification-78d9dc40bf7c
    
    A NN that assesses context (BiGRU feat) while not drowning 
    relevant phrases in said context (CNN feat)
    
    Computes max pooling over the time dimension (which represents
    ordering of words) for both feat representations before passing to
    shallow FFNN for final output
    
    Currently assumes that sentences and paragraphs are BOTH
    just series of words (instead of paragraphs being a series
    of sentences). This makes sense for the CNN as it is aiming
    to model only local dependencies anyway. For the BiGRU,
    it means we don't consider the relationships between sentences
    as a separate concept between that of words- in practice,
    I believe this shouldn't affect us- for example, sentence
    level relations include entailment/contradiction, coherence,
    consistency, etc, all of which feel secondary to meaning for us.
    In fact, we even consider punctuation currently so that's fine too.
    """
    def __init__(
        self, 
        cnn_dense_hidden_dim: int,
        rnn_dense_hidden_dim: int,
        device: str,
        dropout_rate: float = 0.25,
        num_filters: int = 100,
        kernel_sizes: List[int] = [3, 4, 5],
        word_vec_length: int = 300
    ):
        super().__init__()
        self.device = device
        
        self.bigru = BiGRU(
            rnn_dense_hidden_dim, 
            device, 
            dropout_rate, 
            word_vec_length
        )
        self.cnn = textCNN(
            cnn_dense_hidden_dim, 
            device, 
            dropout_rate, 
            num_filters, 
            kernel_sizes, 
            word_vec_length
        )
        self.output = nn.Linear(2, 1)
    
    def forward(self, posts):
        cnn_out = self.cnn(posts)
        bigru_out = self.bigru(posts)
        combined_input = torch.stack((cnn_out, bigru_out), dim=1)
        
        predictionProbs = self.output(combined_input).flatten()
        
        return predictionProbs


#########################################################################
################################ Training ###############################
#########################################################################

def calculate_loss(scores, labels, loss_fn):
    return loss_fn(scores, labels.float())

def get_optimizer(net, lr, weight_decay):
    return optimizer.Adam(net.parameters(), lr=lr, weight_decay = weight_decay)

def get_hyper_parameters():
    cnn_dense_hidden_dim = [256]
    rnn_dense_hidden_dim = [512]
    dropout_rate = [0, 0.1, 0.25]
    lr = [1e-2, 5e-3, 1e-3]
    weight_decay = [0]
    
    return cnn_dense_hidden_dim, rnn_dense_hidden_dim, dropout_rate, lr, weight_decay


def train_model(net, trn_loader, val_loader, optim, num_epoch=50, collect_cycle=30,
        device='cpu', verbose=True, patience=8, stopping_criteria='loss', pos_weight=None):
    train_loss, train_loss_ind, val_loss, val_loss_ind = [], [], [], []
    num_itr = 0
    best_model, best_uar = None, 0

    loss_fn = nn.BCEWithLogitsLoss() if pos_weight is None else nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    early_stopper = EarlyStopperLoss(patience) if stopping_criteria == 'loss' else EarlyStopperAcc(patience)
    if verbose:
        print('------------------------ Start Training ------------------------')
    t_start = time.time()
    for epoch in range(num_epoch):
        # Training:
        net.train()
        for posts, labels in trn_loader:
            num_itr += 1
            posts = [post.to(device) for post in posts]
            labels = labels.to(device)
            
            optim.zero_grad()
            output = net(posts)
            loss = calculate_loss(output, labels, loss_fn)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(net.parameters(), 5)
            optim.step()
            
#             for name, param in net.named_parameters():
#                 print(name, param.grad)
            
            if num_itr % collect_cycle == 0:  # Data collection cycle
                train_loss.append(loss.item())
                train_loss_ind.append(num_itr)
        if verbose:
            print('Epoch No. {0}--Iteration No. {1}-- batch loss = {2:.4f}'.format(
                epoch + 1,
                num_itr,
                loss.item()
                ))

        # Validation:
        uar, accuracy, loss = get_validation_performance(net, loss_fn, val_loader, device)
        val_loss.append(loss)
        val_loss_ind.append(num_itr)
        if verbose:
            print("Validation UAR: {:.4f}".format(uar))
            print("Validation accuracy: {:.4f}".format(accuracy))
            print("Validation loss: {:.4f}".format(loss))
        if uar > best_uar:
            best_model = copy.deepcopy(net)
            best_uar = uar
        if patience is not None and early_stopper.early_stop(
            loss if stopping_criteria == 'loss' else uar
        ):
            break
    
    t_end = time.time()
    if verbose:
        print('Training lasted {0:.2f} minutes'.format((t_end - t_start)/60))
        print('------------------------ Training Done ------------------------')
    stats = {'train_loss': train_loss,
             'train_loss_ind': train_loss_ind,
             'val_loss': val_loss,
             'val_loss_ind': val_loss_ind,
             'accuracy': best_uar,
    }

    return best_model, stats


def get_predictions(scores: torch.Tensor):
    probs = torch.sigmoid(scores)
    return torch.IntTensor([1 if prob > 0.5 else 0 for prob in probs])

def get_validation_performance(net, loss_fn, data_loader, device):
    net.eval()
    y_true = [] # true labels
    y_pred = [] # predicted labels
    total_loss = [] # loss for each batch

    with torch.no_grad():
        for posts, labels in data_loader:
            posts = [post.to(device) for post in posts]
            labels = labels.to(device)
            loss = None # loss for this batch
            pred = None # predictions for this battch

            scores = net(posts)
            loss = calculate_loss(scores, labels, loss_fn)
            pred = torch.IntTensor(get_predictions(scores)).to(device)

            total_loss.append(loss.item())
            y_true.append(labels.cpu())
            y_pred.append(pred.cpu())
    
    y_true = torch.cat(y_true)
    y_pred = torch.cat(y_pred)
    uar = balanced_accuracy_score(y_true, y_pred)
    accuracy = accuracy_score(y_true, y_pred)
    total_loss = sum(total_loss) / len(total_loss)
    
    return uar, accuracy, total_loss


def plot_loss(stats, display=True):
    """Plot training loss and validation loss."""
    plt.plot(stats['train_loss_ind'], stats['train_loss'], label='Training loss')
    plt.plot(stats['val_loss_ind'], stats['val_loss'], label='Validation loss')
    plt.legend()
    plt.xlabel('Number of iterations')
    plt.ylabel('Loss')
    if display:
        plt.show()
    else:
        plt.savefig('best_nn_loss.png')

class EarlyStopperAcc:
    def __init__(self, patience=5):
        self.patience = patience
        self.iters_below = 0
        self.max_acc = -float("inf")

    def early_stop(self, curr_acc):
        if curr_acc > self.max_acc:
            self.max_acc = curr_acc
            self.iters_below = 0
        else:
            self.iters_below += 1
            if self.iters_below >= self.patience:
                return True
        return False

class EarlyStopperLoss:
    # Code inspired from https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch u/isle_of_gods
    def __init__(self, patience=10):
        self.patience = patience
        self.iters_since_last_dec = 0
        self.min_loss = float("inf")

    def early_stop(self, curr_loss):
        if curr_loss < self.min_loss:
            self.min_loss = curr_loss
            self.iters_since_last_dec = 0
        elif curr_loss >= self.min_loss:
            self.iters_since_last_dec += 1
            if self.iters_since_last_dec >= self.patience:
                return True
        return False

(env) [maroofr@gl1023 nn]$ nano gridsearchNN.py 
(env) [maroofr@gl1023 nn]$ python3 gridsearchNN.py cnn
Total arguments passed: 2
Training cnn type model
@@@@@@@@@@@@@@@@@@@@@
@@@@ READING DATAFRAMES AND GLOVE
@@@@@@@@@@@@@@@@@@@@@
/home/maroofr/eecs448-mde/nn/model.py:19: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)
  return {k: torch.FloatTensor(embed[k]) for k in embed.index_to_key}
Done!
@@@@@@@@@@@@@@@@@@@@@
@@@@ CREATING DATASETS AND LOADERS
@@@@@@@@@@@@@@@@@@@@@
Done! Operating on cuda!
@@@@@@@@@@@@@@@@@@@@@
@@@@ EVALUATING HYPERPARAMETERS
@@@@@@@@@@@@@@@@@@@@@
Weighting positive samples tensor([2.6825], device='cuda:0') times more than negative ones
CNN Hidden Size from: [256]
 RNN Hidden Size from: [512]
Learning Rate from: [0.01, 0.005, 0.001]
Weight Decay from: [0]
Dropout Rate from: [0, 0.1, 0.25]
  0%|          | 0/9 [00:00<?, ?it/s]

Beginning assessing cnn hidden 256, rnn hidden 512, drop rate 0, learn rate 0.01 and weight dec 0
Creating textCNN classifier
------------------------ Start Training ------------------------
Epoch No. 1--Iteration No. 977-- batch loss = 1.0343
Validation UAR: 0.5000
Validation accuracy: 0.2716
Validation loss: 1.0095
Epoch No. 2--Iteration No. 1954-- batch loss = 1.1878
Validation UAR: 0.5000
Validation accuracy: 0.7284
Validation loss: 1.0099
Epoch No. 3--Iteration No. 2931-- batch loss = 1.2321
Validation UAR: 0.5000
Validation accuracy: 0.2716
Validation loss: 1.0114
Epoch No. 4--Iteration No. 3908-- batch loss = 0.6833
Validation UAR: 0.5000
Validation accuracy: 0.7284
Validation loss: 1.0114
Epoch No. 5--Iteration No. 4885-- batch loss = 0.8939
Validation UAR: 0.5000
Validation accuracy: 0.7284
Validation loss: 1.0097
Epoch No. 6--Iteration No. 5862-- batch loss = 0.9023
Validation UAR: 0.5000
Validation accuracy: 0.2716
Validation loss: 1.0096
Training lasted 6.42 minutes
------------------------ Training Done ------------------------
Completed (256, 512, 0, 0.01, 0): 0.5

Beginning assessing cnn hidden 256, rnn hidden 512, drop rate 0, learn rate 0.005 and weight dec 0
Creating textCNN classifier
------------------------ Start Training ------------------------
Epoch No. 1--Iteration No. 977-- batch loss = 1.0356
Validation UAR: 0.4973
Validation accuracy: 0.2720
Validation loss: 1.0120
Epoch No. 2--Iteration No. 1954-- batch loss = 0.9626
Validation UAR: 0.5342
Validation accuracy: 0.3403
Validation loss: 1.0051
Epoch No. 3--Iteration No. 2931-- batch loss = 1.1796
Validation UAR: 0.5944
Validation accuracy: 0.5796
Validation loss: 0.9710
Epoch No. 4--Iteration No. 3908-- batch loss = 0.9849
Validation UAR: 0.5865
Validation accuracy: 0.5314
Validation loss: 1.0032
Epoch No. 5--Iteration No. 4885-- batch loss = 0.5243
Validation UAR: 0.5781
Validation accuracy: 0.6403
Validation loss: 1.0891
Epoch No. 6--Iteration No. 5862-- batch loss = 0.4691
Validation UAR: 0.5838
Validation accuracy: 0.6077
Validation loss: 1.2084
Epoch No. 7--Iteration No. 6839-- batch loss = 0.8705
Validation UAR: 0.5709
Validation accuracy: 0.5890
Validation loss: 1.3945
Epoch No. 8--Iteration No. 7816-- batch loss = 0.3771
Validation UAR: 0.5532
Validation accuracy: 0.6649
Validation loss: 2.1747
Training lasted 8.55 minutes
------------------------ Training Done ------------------------
Completed (256, 512, 0, 0.005, 0): 0.594413572086748

Beginning assessing cnn hidden 256, rnn hidden 512, drop rate 0, learn rate 0.001 and weight dec 0
Creating textCNN classifier
------------------------ Start Training ------------------------
Epoch No. 1--Iteration No. 977-- batch loss = 1.0478
Validation UAR: 0.5879
Validation accuracy: 0.4993
Validation loss: 0.9744
Epoch No. 2--Iteration No. 1954-- batch loss = 1.1167
Validation UAR: 0.6038
Validation accuracy: 0.5651
Validation loss: 0.9628
Epoch No. 3--Iteration No. 2931-- batch loss = 0.9474
Validation UAR: 0.5843
Validation accuracy: 0.4938
Validation loss: 0.9816
Epoch No. 4--Iteration No. 3908-- batch loss = 0.6230
Validation UAR: 0.5847
Validation accuracy: 0.5815
Validation loss: 1.0646
Epoch No. 5--Iteration No. 4885-- batch loss = 0.4512
Validation UAR: 0.5671
Validation accuracy: 0.6336
Validation loss: 1.4454
Epoch No. 6--Iteration No. 5862-- batch loss = 0.2316
Validation UAR: 0.5657
Validation accuracy: 0.6393
Validation loss: 1.8577
Epoch No. 7--Iteration No. 6839-- batch loss = 0.4322
Validation UAR: 0.5529
Validation accuracy: 0.6242
Validation loss: 2.4575
Training lasted 7.51 minutes
------------------------ Training Done ------------------------
Completed (256, 512, 0, 0.001, 0): 0.6038082820371977

Beginning assessing cnn hidden 256, rnn hidden 512, drop rate 0.1, learn rate 0.01 and weight dec 0
Creating textCNN classifier
------------------------ Start Training ------------------------
Epoch No. 1--Iteration No. 977-- batch loss = 0.9659
Validation UAR: 0.5000
Validation accuracy: 0.7284
Validation loss: 1.0087
Epoch No. 2--Iteration No. 1954-- batch loss = 0.7521
Validation UAR: 0.5000
Validation accuracy: 0.7284
Validation loss: 1.0097
Epoch No. 3--Iteration No. 2931-- batch loss = 1.0360
Validation UAR: 0.5000
Validation accuracy: 0.2716
Validation loss: 1.0113
Epoch No. 4--Iteration No. 3908-- batch loss = 0.8991
Validation UAR: 0.5000
Validation accuracy: 0.7284
Validation loss: 1.0104
Epoch No. 5--Iteration No. 4885-- batch loss = 1.1657
Validation UAR: 0.5000
Validation accuracy: 0.2716
Validation loss: 1.0106
Epoch No. 6--Iteration No. 5862-- batch loss = 1.1742
Validation UAR: 0.5000
Validation accuracy: 0.7284
Validation loss: 1.0104
Training lasted 6.45 minutes
------------------------ Training Done ------------------------
Completed (256, 512, 0.1, 0.01, 0): 0.5

Beginning assessing cnn hidden 256, rnn hidden 512, drop rate 0.1, learn rate 0.005 and weight dec 0
Creating textCNN classifier
------------------------ Start Training ------------------------
Epoch No. 1--Iteration No. 977-- batch loss = 1.0429
Validation UAR: 0.5137
Validation accuracy: 0.7287
Validation loss: 1.0115
Epoch No. 2--Iteration No. 1954-- batch loss = 1.1173
Validation UAR: 0.5831
Validation accuracy: 0.6506
Validation loss: 0.9838
Epoch No. 3--Iteration No. 2931-- batch loss = 0.8728
Validation UAR: 0.5696
Validation accuracy: 0.6952
Validation loss: 0.9903
Epoch No. 4--Iteration No. 3908-- batch loss = 1.4380
Validation UAR: 0.5383
Validation accuracy: 0.7185
Validation loss: 1.0089
Epoch No. 5--Iteration No. 4885-- batch loss = 0.9333
Validation UAR: 0.5787
Validation accuracy: 0.5499
Validation loss: 1.0013
Epoch No. 6--Iteration No. 5862-- batch loss = 0.9855
Validation UAR: 0.5706
Validation accuracy: 0.6500
Validation loss: 1.0172
Epoch No. 7--Iteration No. 6839-- batch loss = 0.8412
Validation UAR: 0.5607
Validation accuracy: 0.6468
Validation loss: 1.0398
Training lasted 7.52 minutes
------------------------ Training Done ------------------------
Completed (256, 512, 0.1, 0.005, 0): 0.5831138789748057

Beginning assessing cnn hidden 256, rnn hidden 512, drop rate 0.1, learn rate 0.001 and weight dec 0
Creating textCNN classifier
------------------------ Start Training ------------------------
Epoch No. 1--Iteration No. 977-- batch loss = 1.0023
Validation UAR: 0.5870
Validation accuracy: 0.5564
Validation loss: 0.9748
Epoch No. 2--Iteration No. 1954-- batch loss = 0.9341
Validation UAR: 0.5871
Validation accuracy: 0.5365
Validation loss: 0.9732
Epoch No. 3--Iteration No. 2931-- batch loss = 0.7986
Validation UAR: 0.5887
Validation accuracy: 0.6276
Validation loss: 0.9750
Epoch No. 4--Iteration No. 3908-- batch loss = 0.8447
Validation UAR: 0.5862
Validation accuracy: 0.5233
Validation loss: 0.9888
Epoch No. 5--Iteration No. 4885-- batch loss = 0.8126
Validation UAR: 0.5803
Validation accuracy: 0.5979
Validation loss: 1.1040
Epoch No. 6--Iteration No. 5862-- batch loss = 0.5056
Validation UAR: 0.5697
Validation accuracy: 0.5923
Validation loss: 1.2251
Epoch No. 7--Iteration No. 6839-- batch loss = 0.3773
Validation UAR: 0.5541
Validation accuracy: 0.6660
Validation loss: 1.6321
Epoch No. 8--Iteration No. 7816-- batch loss = 0.1954
Validation UAR: 0.5589
Validation accuracy: 0.5942
Validation loss: 1.6721
Training lasted 8.64 minutes
------------------------ Training Done ------------------------
Completed (256, 512, 0.1, 0.001, 0): 0.58873448359487

Beginning assessing cnn hidden 256, rnn hidden 512, drop rate 0.25, learn rate 0.01 and weight dec 0
Creating textCNN classifier
------------------------ Start Training ------------------------
Epoch No. 1--Iteration No. 977-- batch loss = 1.1092
Validation UAR: 0.5000
Validation accuracy: 0.7284
Validation loss: 1.0087
Epoch No. 2--Iteration No. 1954-- batch loss = 1.0349
Validation UAR: 0.5000
Validation accuracy: 0.2716
Validation loss: 1.0098
Epoch No. 3--Iteration No. 2931-- batch loss = 1.2811
Validation UAR: 0.5000
Validation accuracy: 0.2716
Validation loss: 1.0112
Epoch No. 4--Iteration No. 3908-- batch loss = 0.8977
Validation UAR: 0.5000
Validation accuracy: 0.7284
Validation loss: 1.0095
Epoch No. 5--Iteration No. 4885-- batch loss = 1.1083
Validation UAR: 0.5000
Validation accuracy: 0.7284
Validation loss: 1.0114
Epoch No. 6--Iteration No. 5862-- batch loss = 1.1145
Validation UAR: 0.5000
Validation accuracy: 0.7284
Validation loss: 1.0101
Training lasted 6.47 minutes
------------------------ Training Done ------------------------
Completed (256, 512, 0.25, 0.01, 0): 0.5

Beginning assessing cnn hidden 256, rnn hidden 512, drop rate 0.25, learn rate 0.005 and weight dec 0
Creating textCNN classifier
------------------------ Start Training ------------------------
Epoch No. 1--Iteration No. 977-- batch loss = 0.8993
Validation UAR: 0.5018
Validation accuracy: 0.2959
Validation loss: 1.0095
Epoch No. 2--Iteration No. 1954-- batch loss = 0.9643
Validation UAR: 0.5000
Validation accuracy: 0.7284
Validation loss: 1.0091
Epoch No. 3--Iteration No. 2931-- batch loss = 0.8957
Validation UAR: 0.5000
Validation accuracy: 0.7284
Validation loss: 1.0096
Epoch No. 4--Iteration No. 3908-- batch loss = 0.8977
Validation UAR: 0.5000
Validation accuracy: 0.7284
Validation loss: 1.0113
Epoch No. 5--Iteration No. 4885-- batch loss = 1.0353
Validation UAR: 0.5000
Validation accuracy: 0.2716
Validation loss: 1.0096
Epoch No. 6--Iteration No. 5862-- batch loss = 0.8350
Validation UAR: 0.5000
Validation accuracy: 0.2716
Validation loss: 1.0096
Training lasted 6.47 minutes
------------------------ Training Done ------------------------
Completed (256, 512, 0.25, 0.005, 0): 0.5018078415125607

Beginning assessing cnn hidden 256, rnn hidden 512, drop rate 0.25, learn rate 0.001 and weight dec 0
Creating textCNN classifier
------------------------ Start Training ------------------------
Epoch No. 1--Iteration No. 977-- batch loss = 1.0326
Validation UAR: 0.5589
Validation accuracy: 0.7023
Validation loss: 0.9925
Epoch No. 2--Iteration No. 1954-- batch loss = 1.1672
Validation UAR: 0.5924
Validation accuracy: 0.5946
Validation loss: 0.9671
Epoch No. 3--Iteration No. 2931-- batch loss = 1.0083
Validation UAR: 0.5992
Validation accuracy: 0.5704
Validation loss: 0.9685
Epoch No. 4--Iteration No. 3908-- batch loss = 1.0692
Validation UAR: 0.5899
Validation accuracy: 0.5358
Validation loss: 0.9783
Epoch No. 5--Iteration No. 4885-- batch loss = 0.7785
Validation UAR: 0.5831
Validation accuracy: 0.6450
Validation loss: 1.0199
Epoch No. 6--Iteration No. 5862-- batch loss = 0.8308
Validation UAR: 0.5750
Validation accuracy: 0.6349
Validation loss: 1.1175
Epoch No. 7--Iteration No. 6839-- batch loss = 1.1969
Validation UAR: 0.5804
Validation accuracy: 0.5776
Validation loss: 1.1239
Epoch No. 8--Iteration No. 7816-- batch loss = 0.5180
Validation UAR: 0.5763
Validation accuracy: 0.5667
Validation loss: 1.1950
Training lasted 8.63 minutes
------------------------ Training Done ------------------------
Completed (256, 512, 0.25, 0.001, 0): 0.5991753774074255


Best cnn hidden: 256, Best rnn hidden: 512, Best weight_decay: 0, Best lr: 0.001, Best dropout: 0
Accuracy: 0.6038
UAR on test set: 0.5994306247955332, Accuracy on test set: 0.5648878418518898 with loss 0.6806984748326096
(env) [maroofr@gl1023 nn]$ 
