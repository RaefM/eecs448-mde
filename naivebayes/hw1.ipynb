{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bored-graduation",
   "metadata": {},
   "source": [
    "# EECS 487 HW 1: Language Model and Naive Bayes Classifier\n",
    "\n",
    "This notebook contains the programming part of HW 1. In the first problem, you will build two trigram language models for Yelp reviews, one that operates at the word-level and another that operates on the level of individual characters. You will then use these language models to generate sample Yelp reviews. In the second problem, you will build naive bayes classifiers to distinguish between legitimate news headlines and clickbait.\n",
    "\n",
    "After this assignment, you will learn to\n",
    "1. train ngram language models given a text corpus;\n",
    "2. generate text from a language model;\n",
    "3. calculate probability of some text given a language model;\n",
    "4. classify news headlines using naive bayes classifiers;\n",
    "5. evaluate classifiers by calculating the performance on test set.\n",
    "\n",
    "As a reminder, do not edit anything in this python notebook. All the code you need to write are in ```language_model.py``` and ```naive_bayes.py```.\n",
    "\n",
    "## Setup\n",
    "Before we get started, run the following cell to load the autoreload extension so that functions in ```language_model.py``` and ```naive_bayes.py``` will be re-imported into the notebook every time we run them. We also need to import all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "angry-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from language_model import *\n",
    "from naive_bayes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-mason",
   "metadata": {},
   "source": [
    "## 1 N-gram Language Model [40 points]\n",
    "In this problem, you will train two language models on Yelp reviews, one word-level (up to trigrams) and the other character-level (up to 4-grams). The [dataset provided](https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset?resource=download) is a subset of [a larger dataset](https://www.yelp.com/dataset) published by Yelp. We downloaded the file ```yelp.csv``` for you. To begin, you need to first load the data. Here we provide the code for you, but take a look at how we do it because you will need to load the data by yourself later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "enormous-particular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn_text:\n",
      "0        AITA for declining to attend and handle my hal...\n",
      "1        AITA Food Safety in Austin, TexasI work for a ...\n",
      "2        AITA for wanting to have our wedding 2 months ...\n",
      "3        AITA for threatening to kick my wife’s grandma...\n",
      "4        AITA for breaking off a toxic friendshipI met ...\n",
      "                               ...                        \n",
      "78097    AITA for trying to sell an original Xbox for $...\n",
      "78098    WIBTA For communicating discontent to a close ...\n",
      "78099    AITA For being annoyed at my fiance over not d...\n",
      "78100    AITA for calling this guy out for petty dickis...\n",
      "78101    AITA for not wanting my friend bringing a tag ...\n",
      "Name: post, Length: 78102, dtype: object\n",
      "\n",
      "dev_text:\n",
      "0        WIBTA for not wanting to lend my parents money...\n",
      "1        AITA for shutting down my minecraft serverSo i...\n",
      "2        AITA for hiding my mom's car?On mobile, please...\n",
      "3        AITA for not throwing everyone at work a birth...\n",
      "4        AITA for trying to tell my neighbor she needs ...\n",
      "                               ...                        \n",
      "19521    AITA for not wanting my roommate’s cat upstair...\n",
      "19522    AITA for skipping my niece's birthday because ...\n",
      "19523    AITA if I’m upset that my friend jacked off wh...\n",
      "19524    WIBTA for charging friends rent?Good afternoon...\n",
      "19525    AITA for ghosting my good friend from high sch...\n",
      "Name: post, Length: 19526, dtype: object\n"
     ]
    }
   ],
   "source": [
    "filename = 'aita_minimal_preprocess.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "all_text = df['post']\n",
    "\n",
    "trn_text, dev_text = train_test_split(all_text, test_size=0.2, random_state=42)\n",
    "trn_text, dev_text = trn_text.reset_index(drop=True), dev_text.reset_index(drop=True)\n",
    "print(\"trn_text:\")\n",
    "print(trn_text)\n",
    "print(\"\\ndev_text:\")\n",
    "print(dev_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-glance",
   "metadata": {},
   "source": [
    "### 1.1 Data processing and n-gram counts [10 points]\n",
    "Now you need to train your language models on these reviews. You need to implement the class ```NGramLM```. First, **fill in** the function ```get_ngram_counts``` to process the reviews and get the counts of all unigrams, bigrams, trigrams, and possibly four-grams (if character-level) in the reviews. You need to store the counts in the dictionary ```self.ngram_count```. This dictionary will contain dictionaries as values. For example, ```self.ngram_count[0]``` will be a dictionary containing all of the unigrams, and ```self.ngram_count[1]``` will be a dictionary containing all the bigrams.\n",
    "\n",
    "To access the count of a unigram, simply use it as a key in the unigram dictionary: ```self.ngram_count[0][\"word1\"]``` will be $C(word1)$.\n",
    "\n",
    "To access the count of a bigram (or trigram or four-gram), simply use a tuple: ```self.ngram_count[1][(\"word1\", \"word2\")]``` will be $C(word1, word2)$.\n",
    "\n",
    "Use the following rules when processing the reviews:\n",
    "- Prepend **two/three** &lt;s&gt; at the beginning of each review as BOS tokens (two for word-level model and three for character-level model), and append one &lt;/s&gt; at the end of a review as the EOS token. \n",
    "- Convert all letters to lowercase.\n",
    "- Tokenize each review. Use a word-level tokenizer (such as ```nltk.tokenize.word_tokenize```) for the word-level model and a character-level tokenizer (such as ```char_tokenizer```, defined below) for the character-level model. Do not split BOS and EOS tokens.\n",
    "- You will need to make the function ```get_ngram_counts``` flexible enough so that (1) it can operate on both the character-level and the word-level and (2) it can operate on a variety of ngram sizes (e.g. trigrams and four-grams)\n",
    "- Replace all tokens that occur < 2 times with \"UNK\". Note: for the character-level model, this means that all characters occuring less than once should be replaced with UNK.\n",
    "- Do **NOT** remove punctuation.\n",
    "\n",
    "Hint: ```collections.defaultdict``` is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "superb-sympathy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unigrams: 58214\n",
      "Unigram with smallest count: trickster\tCount: 2\n",
      "Unknown unigram: 75638\n",
      "Number of BOS token: 156204\n",
      "Number of bigrams: 2058184\n"
     ]
    }
   ],
   "source": [
    "bos_token, eos_token = '<s>', '</s>'\n",
    "ngram_size = 3 # Use trigrams\n",
    "word_lm = NGramLM(bos_token, eos_token, word_tokenize, ngram_size)\n",
    "word_lm.get_ngram_counts(trn_text.tolist())\n",
    "print(f\"Number of unigrams: {len(word_lm.ngram_count[0])}\")\n",
    "least_unigram = min(word_lm.ngram_count[0].keys(), key=lambda x: word_lm.ngram_count[0][x])\n",
    "print(f\"Unigram with smallest count: {least_unigram}\\tCount: {word_lm.ngram_count[0][least_unigram]}\")\n",
    "print(f\"Unknown unigram: {word_lm.ngram_count[0]['UNK']}\")\n",
    "print(f\"Number of BOS token: {word_lm.ngram_count[0][bos_token]}\")\n",
    "print(f\"Number of bigrams: {len(word_lm.ngram_count[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef26743",
   "metadata": {},
   "source": [
    "Now that you have tested ```get_ngram_counts``` on the word-level model (above), test it on the character-level model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ethical-giant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unigrams: 83\n",
      "Unigram with smallest count: ü\tCount: 2\n",
      "Unknown unigram: 8\n",
      "Number of BOS token: 24000\n",
      "Number of bigrams: 2642\n"
     ]
    }
   ],
   "source": [
    "def char_tokenizer(text):\n",
    "    return [char for char in text]\n",
    "\n",
    "ngram_size = 4 # Use four-grams\n",
    "char_lm = NGramLM(bos_token, eos_token, char_tokenizer, ngram_size)\n",
    "char_lm.get_ngram_counts(trn_text.tolist())\n",
    "print(f\"Number of unigrams: {len(char_lm.ngram_count[0])}\")\n",
    "least_unigram = min(char_lm.ngram_count[0].keys(), key=lambda x: char_lm.ngram_count[0][x])\n",
    "print(f\"Unigram with smallest count: {least_unigram}\\tCount: {char_lm.ngram_count[0][least_unigram]}\")\n",
    "print(f\"Unknown unigram: {char_lm.ngram_count[0]['UNK']}\")\n",
    "print(f\"Number of BOS token: {char_lm.ngram_count[0][bos_token]}\")\n",
    "print(f\"Number of bigrams: {len(char_lm.ngram_count[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-charge",
   "metadata": {},
   "source": [
    "### 1.2 Add-k Smoothing [5 points]\n",
    "As discussed in the lecture, simply counting the number of occurrence of n-grams will assign 0 probability to n-grams that don't appear in the training corpus and thus cannot generalize to unseen data. To mitigate this, you need to implement some smoothing techniques. **Fill in** the function ```add_k_prob``` so that given a bigram $(w_1, w_2)$, a unigram $w_3$, and $k$, return $p(w_3|(w_1, w_2))$ after applying add-k smoothing.<br>\n",
    "\n",
    "Notes: \n",
    "- Program this flexibly enough so that, for the character-level model, the model can smooth over trigrams and unigrams rather than bigrams and unigrams.\n",
    "- &lt;s&gt; should **NOT** be considered when calculating the vocabulary size because it will never be generated by the language model (although it's in ```self.unigram_count```). &lt;/s&gt; should be treated as a token in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "substantial-trust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['<s>', 'UNK', 'apple', '</s>', 'circle', 'dot', 'triangle', 'the', 'red', 'blue']\n",
      "Probability of seen: 0.23076923076923078\n",
      "Probability of unseen: 0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "# prepare counts\n",
    "test_data = ['An apple', 'A circle', 'This dot', 'That triangle', 'The red apple', 'The red circle', 'The blue dot', 'The blue triangle']\n",
    "k = 0.5\n",
    "test_word_lm = NGramLM(bos_token, eos_token, word_tokenize, 3)\n",
    "test_word_lm.get_ngram_counts(test_data)\n",
    "print(f\"Vocabulary: {list(test_word_lm.ngram_count[0].keys())}\")\n",
    "# test\n",
    "bigram = ('the', 'red')\n",
    "unigram = 'apple'\n",
    "print(f\"Probability of seen: {test_word_lm.add_k_smooth_prob(bigram, unigram, k)}\")\n",
    "\n",
    "bigram = ('the', 'blue')\n",
    "unigram = 'apple'\n",
    "print(f\"Probability of unseen: {test_word_lm.add_k_smooth_prob(bigram, unigram, k)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf1040d",
   "metadata": {},
   "source": [
    "Now test out the same smoothing code on the character-level model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unexpected-status",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['<s>', 'p', 'l', 'a', 'n', '</s>', 't', 'UNK']\n",
      "Probability of seen: 0.5384615384615384\n",
      "Probability of unseen: 0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "test_data = ['plan', 'plant', 'planet']\n",
    "k = 0.5\n",
    "test_char_lm = NGramLM(bos_token, eos_token, char_tokenizer, 4)\n",
    "test_char_lm.get_ngram_counts(test_data)\n",
    "print(f\"Vocabulary: {list(test_char_lm.ngram_count[0].keys())}\")\n",
    "# test\n",
    "trigram = ('p', 'l', 'a')\n",
    "unigram = 'n'\n",
    "print(f\"Probability of seen: {test_char_lm.add_k_smooth_prob(trigram, unigram, k)}\")\n",
    "\n",
    "trigram = ('p', 'l', 'a')\n",
    "unigram = 't'\n",
    "print(f\"Probability of unseen: {test_char_lm.add_k_smooth_prob(trigram, unigram, k)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-respondent",
   "metadata": {},
   "source": [
    "### 1.3 Linear interpolation [4 points]\n",
    "Similarly, **fill in** the function ```linear_interp_prob``` so that given a bigram $(w_1, w_2)$, a unigram $w_3$, and list of values [$\\lambda_1$, $\\lambda_2$, $\\lambda_3$], return $p(w_3|(w_1, w_2))$ after applying linear interpolation.\n",
    "\n",
    "Once again, implement this flexibly enough to operate on four-grams for the character-level model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "latest-summit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of seen: 0.4142857142857143\n",
      "Probability of unseen: 0.014285714285714285\n"
     ]
    }
   ],
   "source": [
    "lambda1 = 0.6\n",
    "lambda2 = 0.2\n",
    "lambda3 = 0.2\n",
    "lambdas = [lambda1, lambda2, lambda3]\n",
    "\n",
    "bigram = ('the', 'red')\n",
    "unigram = 'apple'\n",
    "print(f\"Probability of seen: {test_word_lm.linear_interp_prob(bigram, unigram, lambdas)}\")\n",
    "\n",
    "bigram = ('the', 'blue')\n",
    "unigram = 'apple'\n",
    "print(f\"Probability of unseen: {test_word_lm.linear_interp_prob(bigram, unigram, lambdas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86807e66",
   "metadata": {},
   "source": [
    "Now test ```linear_interp_prob``` on the character-level model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "accessible-debut",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of seen: 0.9166666666666667\n",
      "Probability of unseen: 0.011111111111111112\n"
     ]
    }
   ],
   "source": [
    "lambda1 = 0.6\n",
    "lambda2 = 0.2\n",
    "lambda3 = 0.1\n",
    "lambda4 = 0.1\n",
    "lambdas = [lambda1, lambda2, lambda3, lambda4]\n",
    "\n",
    "trigram = ('p', 'l', 'a')\n",
    "unigram = 'n'\n",
    "print(f\"Probability of seen: {test_char_lm.linear_interp_prob(trigram, unigram, lambdas)}\")\n",
    "\n",
    "trigram = ('p', 'l', 'a')\n",
    "unigram = 't'\n",
    "print(f\"Probability of unseen: {test_char_lm.linear_interp_prob(trigram, unigram, lambdas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-cambridge",
   "metadata": {},
   "source": [
    "### 1.4 Calculate next word probability [2 points]\n",
    "**Fill in** the function ```get_probability``` that calculates $p(w_3|(w_1, w_2))$ using either add-k smoothing or linear interpolation that you implemented above. The input is a dictionary that specifies how should you do the smoothing. Once again, program this function flexibly so that it works in the character-level model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "trying-delivery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add-k smoothing: 0.1111111111111111\n",
      "Linear interpolation: 0.014285714285714285\n"
     ]
    }
   ],
   "source": [
    "smoothing_args1 = {\n",
    "    'method': 'add_k',\n",
    "    'k': 0.5\n",
    "}\n",
    "smoothing_args2 = {\n",
    "    'method': 'linear',\n",
    "    'lambdas': [0.6, 0.2, 0.2]\n",
    "}\n",
    "bigram = ('a', 'red')\n",
    "unigram = 'dot'\n",
    "print(f\"Add-k smoothing: {test_word_lm.get_probability(bigram, unigram, smoothing_args1)}\")\n",
    "print(f\"Linear interpolation: {test_word_lm.get_probability(bigram, unigram, smoothing_args2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-singer",
   "metadata": {},
   "source": [
    "### 1.5 Calculate perplexity [4 points]\n",
    "One way to evaluate the language model is to calculate its perplexity on some validation data. **Fill in** the function ```get_perplexity``` so that given a document and smoothing arguments, return the perplexity of the document. Remember to follow the **same** processing steps you used previously. To avoid underflow issue, remember to calculate the log of perplexity first. i.e., $PPL(W)=\\exp\\left(\\log\\left(\\sqrt[N]{\\prod_{i=1}^N{\\frac{1}{p(w_i|w_{i-2}w_{i-1})}}}\\right)\\right)=\\exp\\left(-\\frac{1}{N}\\sum_{i=1}^N{\\log{p(w_i|w_{i-2}w_{i-1})}}\\right)$, where $w_0=w_{-1}=$ &lt;s&gt; and $w_N=$ &lt;/s&gt;. Also, once again, remember to program this flexibly enough that it can work for both the word-level and character-level models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hawaiian-examination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add-k smoothing: 8.332312830191487\n",
      "Linear interpolation: 21.91528915573818\n"
     ]
    }
   ],
   "source": [
    "text = \"This sentence contains unseen words.\"\n",
    "\n",
    "print(f\"Add-k smoothing: {test_word_lm.get_perplexity(text, smoothing_args1)}\")\n",
    "print(f\"Linear interpolation: {test_word_lm.get_perplexity(text, smoothing_args2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-correlation",
   "metadata": {},
   "source": [
    "### 1.6 Search hyperparameters [6 points]\n",
    "Now you are ready to find your best language models! First find the best k value for the word-level model and for the character-level model using add-k smoothing. You need to search k in this list: \\[0.2, 0.4, 0.6, 0.8, 1.0\\].\n",
    "\n",
    "**Fill in** the function ```search_k``` such that given a validation set, return the best k value on it. Print out the perplexity (average on the whole validation set) for each k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acceptable-dealer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word LM\n",
      "0.2: 2707.280067607868\n",
      "0.4: 3511.3579280601293\n",
      "0.6: 4093.656248471366\n",
      "0.8: 4558.66073583255\n",
      "1: 4948.407883582781\n",
      "Best k: 0.2\n",
      "Char LM\n",
      "0.2: 4.851977522034394\n",
      "0.4: 4.91823073361118\n",
      "0.6: 4.979052820932485\n",
      "0.8: 5.035300360620784\n",
      "1: 5.087992872969051\n",
      "Best k: 0.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Word LM\")\n",
    "word_k = word_lm.search_k(dev_text)\n",
    "print(f\"Best k: {word_k}\")\n",
    "print(\"Char LM\")\n",
    "char_k = char_lm.search_k(dev_text)\n",
    "print(f\"Best k: {char_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-theorem",
   "metadata": {},
   "source": [
    "Similarly, **fill in** the function ```search_lambda``` such that, given a validation set, returns the best $\\lambda$ values on it. You need to choose the search list by yourself. Print out the best set of $\\lambda$ and corresponding perplexity. To get full credits, your perplexity scores need to be < 180 for the word-level model and < 15 for the character-level model.\n",
    "\n",
    "Note: this code block might take a couple minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "blocked-monaco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word LM\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord LM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m word_lambda \u001b[38;5;241m=\u001b[39m \u001b[43mword_lm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_lambda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print(\"Char LM\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# char_lambda = char_lm.search_lambda(dev_text)\u001b[39;00m\n",
      "File \u001b[0;32m~/class/448/mde/naivebayes/language_model.py:271\u001b[0m, in \u001b[0;36mNGramLM.search_lambda\u001b[0;34m(self, dev_data)\u001b[0m\n\u001b[1;32m    268\u001b[0m pp_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m dev_data:\n\u001b[0;32m--> 271\u001b[0m     pp_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmethod\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlambdas\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_lambdas\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m pp_avg \u001b[38;5;241m=\u001b[39m pp_sum \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dev_data)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_pp_avg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m pp_avg \u001b[38;5;241m<\u001b[39m best_pp_avg:\n",
      "File \u001b[0;32m~/class/448/mde/naivebayes/language_model.py:206\u001b[0m, in \u001b[0;36mNGramLM.get_perplexity\u001b[0;34m(self, text, smoothing_args)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenized_text_with_unk:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbos_token:\n\u001b[0;32m--> 206\u001b[0m         prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_probability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecursors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothing_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m         log_prob_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(prob)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# adjust precursor set so next has 'num_precursors' tokens before it\u001b[39;00m\n",
      "File \u001b[0;32m~/class/448/mde/naivebayes/language_model.py:181\u001b[0m, in \u001b[0;36mNGramLM.get_probability\u001b[0;34m(self, n_minus1_gram, unigram, smoothing_args)\u001b[0m\n\u001b[1;32m    179\u001b[0m     probability \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_k_smooth_prob(n_minus1_gram, unigram, smoothing_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m     probability \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_interp_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_minus1_gram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munigram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothing_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlambdas\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m#################################################################################\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probability\n",
      "File \u001b[0;32m~/class/448/mde/naivebayes/language_model.py:167\u001b[0m, in \u001b[0;36mNGramLM.linear_interp_prob\u001b[0;34m(self, n_minus1_gram, unigram, lambdas)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, lambda_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lambdas):\n\u001b[1;32m    166\u001b[0m     num_prec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(lambdas) \u001b[38;5;241m-\u001b[39m i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 167\u001b[0m     probability \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m lambda_i \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpr_unigram_given_last_j_of_ngram\u001b[49m\u001b[43m(\u001b[49m\u001b[43munigram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_minus1_gram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_prec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m#################################################################################\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probability\n",
      "File \u001b[0;32m~/class/448/mde/naivebayes/language_model.py:123\u001b[0m, in \u001b[0;36mNGramLM.pr_unigram_given_last_j_of_ngram\u001b[0;34m(self, unigram, ngram, j, k)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# if condition is empty (aka if calculating unigrams standalone probability)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# count of unigram frequency over the total number of tokens in corpus (not unique and w/o <s>)\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     total_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngram_count\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_count[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbos_token]\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unigram \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_count[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_count[\u001b[38;5;241m0\u001b[39m][unigram] \u001b[38;5;241m/\u001b[39m total_tokens\n\u001b[1;32m    126\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_count[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Word LM\")\n",
    "word_lambda = word_lm.search_lambda(dev_text)\n",
    "# print(\"Char LM\")\n",
    "# char_lambda = char_lm.search_lambda(dev_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9616083",
   "metadata": {},
   "source": [
    "### 1.7 Generate reviews [5 points]\n",
    "Finally, you can automatically generate text using your language models. **Fill in** the function ```generate_text``` to generate a sentence based on an input prompt. To generate the text, you need to find the distribution of the next word given previous two words (or next character given the previous three characters). Then you need to sample the next word/character based on the distribution. After the word/character is sampled, append it to the current text and continue generating the next word/character. You need to repeat this process untill the current sequence **has 15 words/characters** (including prompts) or you **generate the &lt;/s&gt; token**.\n",
    "\n",
    "Note that there exist more advanced methods to generate text from language models such as beam search, top-k sampling, and top-p sampling. You can refer to [this blog](https://huggingface.co/blog/how-to-generate) to get an idea of what they mean. In this assignment, you are not required to implement the advanced methods. Simply sampling from the trigram/four-gram distribution is good enough.\n",
    "\n",
    "Begin with the word-level model here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb9d1e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word LM\n",
      "the location brows ebay sounds physical pumpkin kheer rooste superbowl 5:00pm pours sopapilla dulce advance\n",
      "Word LM\n",
      "we ate topic inedible unheard cant landlocked carry-out selections giggle habit subliminal cin goldfish pieces\n",
      "Word LM\n",
      "i thought trying packets fights round-trip overpriced apartments sands grade shipped grinders 69 ushered coyotes\n",
      "Word LM\n",
      "it had 'bertos beautiful alexa simply sitting appreciate deadline passed bottomless bosa kids madden camaraderie\n"
     ]
    }
   ],
   "source": [
    "prompts = [['The', 'location'], ['We', 'ate'], ['I', 'thought'], ['It', 'had']]\n",
    "for prompt in prompts:\n",
    "    print(\"Word LM\")\n",
    "    word_lm.generate_text(prompt, smoothing_args1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b7b51a",
   "metadata": {},
   "source": [
    "Now test the character-level model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0774fc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char LM\n",
      "t h e   s t o r e   i t ' s  \n",
      "Char LM\n",
      "w e   a t e r   l u n c h .  \n"
     ]
    }
   ],
   "source": [
    "prompts = ['The store', 'We ate']\n",
    "for prompt in prompts:\n",
    "    print(\"Char LM\")\n",
    "    prompt_tokenized = char_tokenizer(prompt)\n",
    "    char_lm.generate_text(prompt_tokenized, smoothing_args1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-dining",
   "metadata": {},
   "source": [
    "### 1.8 Predict the class of a review [4 points]\n",
    "The Yelp review dataset contains several columns specifying attributes of each review:\n",
    "- ```stars``` indicates on a scale from 1 to 5 how many stars the reviewer gave in the review\n",
    "- ```cool```, ```useful```, and ```funny``` all indicate how many users rated the review as cool, useful or funny\n",
    "\n",
    "Using the same techniques developed above, this section requires you to do the following:\n",
    "- Fill in the function ```load_new_data``` to generate a split in the data. For instance, one class could be reviews with at least one funny rating and the opposing class could be reviews not marked as funny by anyone. In this function, use the techniques at the beginning of the notebook to split the overall dataset into two classes and then again into a training and dev dataset for each class\n",
    "- Fill in the function ```predict_class``` to predict the class to which the review in ```test_review.txt``` belongs. This function will take in two word-level language models (each trained on the training data for one class) and compute the probability of the review generated by each language model. I.e., $p(W|LM_1)$ and $p(W|LM_2)$, or equivalently $PPL(W)$ based on $LM_1$ and $LM_2$. Print out the perplexity of each language model and your prediction.\n",
    "\n",
    "The test review used has the following attributes:\n",
    "- ```stars``` : 2\n",
    "- ```useful``` : 3\n",
    "- ```funny``` : 7\n",
    "- ```cool``` : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aware-craft",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>Let's see...what is there NOT to like about Su...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>First visit...Had lunch here today - used my G...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Should be called house of deliciousness!\\n\\nI ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>I recently visited Olive and Ivy for business ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8324 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  stars\n",
       "0     My wife took me here on my birthday for breakf...      5\n",
       "1     I have no idea why some people give bad review...      5\n",
       "2     love the gyro plate. Rice is so good and I als...      4\n",
       "3     Rosie, Dakota, and I LOVE Chaparral Dog Park!!...      5\n",
       "4     General Manager Scott Petello is a good egg!!!...      5\n",
       "...                                                 ...    ...\n",
       "9994  Let's see...what is there NOT to like about Su...      5\n",
       "9995  First visit...Had lunch here today - used my G...      3\n",
       "9996  Should be called house of deliciousness!\\n\\nI ...      4\n",
       "9997  I recently visited Olive and Ivy for business ...      4\n",
       "9999  4-5 locations.. all 4.5 star average.. I think...      5\n",
       "\n",
       "[8324 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Was it worth the 21$ for a salad and small piz...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>U can go there n check the car out. If u wanna...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Disgusting!  Had a Groupon so my daughter and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>I've eaten here many times, but none as bad as...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>I have to add to Andrew's review.....\\n\\nI jus...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9954</th>\n",
       "      <td>Stopped here for lunch with my BFF today.\\n\\nN...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>If Cowboy Ciao is the best restaurant in Scott...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>Went last night to Whore Foods to get basics t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>The food is delicious.  The service:  discrimi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>My nephew just moved to Scottsdale recently so...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1676 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  stars\n",
       "15    Was it worth the 21$ for a salad and small piz...      2\n",
       "23    U can go there n check the car out. If u wanna...      1\n",
       "31    Disgusting!  Had a Groupon so my daughter and ...      1\n",
       "35    I've eaten here many times, but none as bad as...      1\n",
       "56    I have to add to Andrew's review.....\\n\\nI jus...      2\n",
       "...                                                 ...    ...\n",
       "9954  Stopped here for lunch with my BFF today.\\n\\nN...      2\n",
       "9974  If Cowboy Ciao is the best restaurant in Scott...      2\n",
       "9984  Went last night to Whore Foods to get basics t...      1\n",
       "9987  The food is delicious.  The service:  discrimi...      1\n",
       "9998  My nephew just moved to Scottsdale recently so...      2\n",
       "\n",
       "[1676 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for class1_lm: 109.32558016309689\n",
      "Perplexity for class2_lm: 125.94953883791348\n",
      "It is in class 1\n"
     ]
    }
   ],
   "source": [
    "filename = 'yelp.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "smoothing_args = {\n",
    "    'method': 'linear',\n",
    "    'lambdas': word_lambda\n",
    "}\n",
    "\n",
    "class1_trn, class1_dev, class2_trn, class2_dev = load_new_data(df)\n",
    "\n",
    "class1_lm = NGramLM(bos_token, eos_token, word_tokenize, 3)\n",
    "class1_lm.get_ngram_counts(class1_trn)\n",
    "\n",
    "class2_lm = NGramLM(bos_token, eos_token, word_tokenize, 3)\n",
    "class2_lm.get_ngram_counts(class2_trn)\n",
    "\n",
    "predict_class('test_review.txt', class1_lm, class2_lm, smoothing_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-poison",
   "metadata": {},
   "source": [
    "## 2 Naive Bayes for Text Classification [29 points]\n",
    "In this problem, you will build naive bayes classifiers to do text classification. You will use the clickbait headlines dataset, which contains examples of legitimate news headlines and clickbait news headlines. The original dataset can be found in [this GitHub repository](https://github.com/bhargaviparanjape/clickbait) and [this paper](https://arxiv.org/abs/1610.09786).\n",
    "### 2.1 Load dataset [4 points]\n",
    "To get started, **fill in** the function ```load_headlines``` to load the clickbait dataset into pandas dataframes. The file ```clickbait_data.csv``` contains a partially processed subset of the data. It contains two columns: (1) ```is_clickbait``` is 1 when the row contains a clickbait headline and 0 when it doesn't and (2) ```text```, which contains the headline itself.\n",
    "\n",
    "To get started, **fill in** the function ```load_headlines``` to load the clickbait dataset into a pandas dataframe. To do this, you will need to do the following:\n",
    "\n",
    "1. Read in the ```text``` and ```is_clickbait``` columns.\n",
    "2. Rename the ```is_clickbait``` column to ```label```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "realistic-execution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6152</th>\n",
       "      <td>1</td>\n",
       "      <td>AITA for getting in a fight at a funeral?Full ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57628</th>\n",
       "      <td>1</td>\n",
       "      <td>AITA for getting my dog high before a bath????...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17776</th>\n",
       "      <td>1</td>\n",
       "      <td>AITA for telling my sister's boyfriend the tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48015</th>\n",
       "      <td>1</td>\n",
       "      <td>AITA For Ignoring Kids making Small TalkI am n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30766</th>\n",
       "      <td>1</td>\n",
       "      <td>AITA for telling my boyfriend he sits on his a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5633</th>\n",
       "      <td>0</td>\n",
       "      <td>AITA for being fed up with my girlfriends ment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23040</th>\n",
       "      <td>0</td>\n",
       "      <td>AITA for continuing to date my boyfriend even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12025</th>\n",
       "      <td>0</td>\n",
       "      <td>AITA for not apologizing to my fatherHi, 18f h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56946</th>\n",
       "      <td>0</td>\n",
       "      <td>AITA for making my boyfriends coworkers wait o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96806</th>\n",
       "      <td>0</td>\n",
       "      <td>AITA for calling out a catfish?There’s a girl ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78102 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "6152       1  AITA for getting in a fight at a funeral?Full ...\n",
       "57628      1  AITA for getting my dog high before a bath????...\n",
       "17776      1  AITA for telling my sister's boyfriend the tru...\n",
       "48015      1  AITA For Ignoring Kids making Small TalkI am n...\n",
       "30766      1  AITA for telling my boyfriend he sits on his a...\n",
       "...      ...                                                ...\n",
       "5633       0  AITA for being fed up with my girlfriends ment...\n",
       "23040      0  AITA for continuing to date my boyfriend even ...\n",
       "12025      0  AITA for not apologizing to my fatherHi, 18f h...\n",
       "56946      0  AITA for making my boyfriends coworkers wait o...\n",
       "96806      0  AITA for calling out a catfish?There’s a girl ...\n",
       "\n",
       "[78102 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22132</th>\n",
       "      <td>0</td>\n",
       "      <td>AITA for not babysitting a family member’s tod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78836</th>\n",
       "      <td>1</td>\n",
       "      <td>AITA for not ‘Sharing the wealth’For one thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7265</th>\n",
       "      <td>0</td>\n",
       "      <td>AITA for cancelling my own wedding reception a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89507</th>\n",
       "      <td>0</td>\n",
       "      <td>WIBTA If I offered my Friends BF my Shoes?So I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86350</th>\n",
       "      <td>0</td>\n",
       "      <td>AITA for telling my friend to not ask me to ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54017</th>\n",
       "      <td>0</td>\n",
       "      <td>AITA for refusing to go to my sister’s 18th?Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79438</th>\n",
       "      <td>0</td>\n",
       "      <td>AITA for telling my sister in law I don’t to u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31911</th>\n",
       "      <td>0</td>\n",
       "      <td>AITA for telling my mom off in defense of my s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19308</th>\n",
       "      <td>0</td>\n",
       "      <td>AITA - Don't bring up grad school!I recently s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31271</th>\n",
       "      <td>1</td>\n",
       "      <td>AITA I Accidentally Kissed My FriendSo basical...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19526 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "22132      0  AITA for not babysitting a family member’s tod...\n",
       "78836      1  AITA for not ‘Sharing the wealth’For one thing...\n",
       "7265       0  AITA for cancelling my own wedding reception a...\n",
       "89507      0  WIBTA If I offered my Friends BF my Shoes?So I...\n",
       "86350      0  AITA for telling my friend to not ask me to ha...\n",
       "...      ...                                                ...\n",
       "54017      0  AITA for refusing to go to my sister’s 18th?Ba...\n",
       "79438      0  AITA for telling my sister in law I don’t to u...\n",
       "31911      0  AITA for telling my mom off in defense of my s...\n",
       "19308      0  AITA - Don't bring up grad school!I recently s...\n",
       "31271      1  AITA I Accidentally Kissed My FriendSo basical...\n",
       "\n",
       "[19526 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_data = load_posts('aita_minimal_preprocess.csv')\n",
    "\n",
    "(train, test) = train_test_split(all_data, train_size=0.8)\n",
    "\n",
    "display(train)\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-spank",
   "metadata": {},
   "source": [
    "### 2.2 Dataset statistics [3 points]\n",
    "Before start training classifiers, you need to calculate some basic statistics of the dataset. **Fill in** the function ```get_basic_stats``` to print out the following statistics of the training data:\n",
    "- Average number of tokens per headline\n",
    "- Standard deviation of the number of tokens per headline\n",
    "- Total number of legitimate headlines\n",
    "- Total number of clickbait headlines\n",
    "\n",
    "Note: you can use any tokenization method you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sticky-account",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation: 214.36447117694817\n",
      "Number of legitimate/clickbait headlines: {0: 56835, 1: 21267}\n"
     ]
    }
   ],
   "source": [
    "get_basic_stats(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-rental",
   "metadata": {},
   "source": [
    "### 2.3 Data processing and ngram calculation [6 points]\n",
    "Now you need to calculate the ngram counts. **Fill in** the function ```fit``` that, given a dataframe of training data, calculates the ngram counts in each category and the prior probability for each category. Concretely, **store** the total occurrence of each ngram in each category in a list called ```self.ngram_count``` so that ```self.ngram_count[0]``` contains $count(w, c_0)$ for all $w$ in the vocabulary, and ```self.ngram_count[1]``` contains $count(w, c_1)$, etc. ```self.ngram_count[i]``` should be an array of shape $(1,|V|)$, where $V$ is the vocabulary. **Store** the total occurrence of all ngrams in each category in a list called ```self.total_count``` so that ```self.total_count[0]``` $=\\sum_{w\\in V}count(w, c_0)$, and ```self.total_count[1]``` $=\\sum_{w\\in V}count(w, c_1)$, etc. **Store** the prior probability for each category in ```self.category_prob```. You need to follow these rules when calculating the counts:\n",
    "- convert all letters to lowercase;\n",
    "- include both unigrams and bigrams;\n",
    "- ignore terms that appear in more than 80\\% of the headlines;\n",
    "- ignore terms that appear in less than 3 headlines.\n",
    "\n",
    "Hint: use ```CountVectorizer``` in sklearn and store it as ```self.vectorizer```. You need to use **both legitimate and clickbait headlines** to get the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = NaiveBayes()\n",
    "naive_bayes.fit(train)\n",
    "print(f\"Probability for each category: {naive_bayes.category_prob}\")\n",
    "print(f\"Length of self.ngram_count: {len(naive_bayes.ngram_count)}\")\n",
    "print(f\"Shape of the counts for 1st category: {naive_bayes.ngram_count[0].shape}\")\n",
    "print(f\"Number of non-zero terms for 1st category: {(naive_bayes.ngram_count[0] > 0).sum()}\")\n",
    "print(f\"Maximum count of the 1st category: {naive_bayes.ngram_count[0].max()}\")\n",
    "print(f\"Minimum count of the 1st category: {naive_bayes.ngram_count[0].min()}\")\n",
    "print(f\"Sum of ngram count for 1st category: {naive_bayes.ngram_count[0].sum()}\")\n",
    "print(f\"Total count for each category: {naive_bayes.total_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-decimal",
   "metadata": {},
   "source": [
    "### 2.4 Calculate posterior probability for a category [4 points]\n",
    "Next, you will use the vectorizer and ngram counts to calculate the posterior probability of a category. In this homework, we have two categories: legitimate and clickbait. **Fill in** the function ```calculate_prob``` that given a list of articles $docs$, a category index $i$, return $\\log\\left(p(c_i)p(d|c_i)\\right)=\\log\\left(p(c_i)\\prod_{x\\in X}p(x|c_i)\\right)$ for each article $d$ in $docs$, where $X$ is the set of unigrams and bigrams in **both** article $d$ and vocabulary $V$. Use **add-one smoothing** in your calculation and calculate the **sum of log** to avoid underflow issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "brave-determination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability for category 0: [-62.62606394579369, -69.86159317661772]\n",
      "Probability for category 1: [-77.82936654650696, -66.7894093193031]\n"
     ]
    }
   ],
   "source": [
    "test_docs = [\"He was such a rude jerk, I never want to be near him again\",\n",
    " \"People don't understand that I'm ACTUALLY a genius\"]\n",
    "prob1 = naive_bayes.calculate_prob(test_docs, 0)\n",
    "prob2 = naive_bayes.calculate_prob(test_docs, 1)\n",
    "print(f\"Probability for category 0: {prob1}\")\n",
    "print(f\"Probability for category 1: {prob2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-sheep",
   "metadata": {},
   "source": [
    "### 2.5 Predict labels for new headlines [2 points]\n",
    "With the posterior probability of each category, you can predict the label for new headlines. **Fill in** the function ```predict``` that, given a list of headlines, returns the predicted categories of the headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "opposed-territory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0 1]\n"
     ]
    }
   ],
   "source": [
    "preds = naive_bayes.predict(test_docs)\n",
    "print(f\"Prediction: {preds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-semester",
   "metadata": {},
   "source": [
    "### 2.6 Calculate evaluation metrics [5 points]\n",
    "To evaluate a classifier, you need to calculate some evaluation metrics. **Fill in** the function ```evaluate``` that, given a list of predictions and a list of true labels, returns the accuracy, macro f1-score, and micro f1-score. You can **NOT** use functions in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "level-casino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7142857142857143\n",
      "Macro f1: 0.7083333333333333\n",
      "Micro f1: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "predictions = [1,1,0,1,0,0,1]\n",
    "labels = [1,0,0,1,0,1,1]\n",
    "accuracy, mac_f1, mic_f1 = evaluate(predictions, labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Macro f1: {mac_f1}\")\n",
    "print(f\"Micro f1: {mic_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-corner",
   "metadata": {},
   "source": [
    "### 2.7 Test classifier on test data [2 points]\n",
    "Finally, you are ready to evaluate your classifier on the test data! Run the following cell to make predictions and print out performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "amended-angle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.961\n",
      "Macro f1: 0.9609934078859327\n",
      "Micro f1: 0.961\n"
     ]
    }
   ],
   "source": [
    "predictions = naive_bayes.predict(test.text.tolist())\n",
    "labels = test.label.tolist()\n",
    "accuracy, mac_f1, mic_f1 = evaluate(predictions, labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Macro f1: {mac_f1}\")\n",
    "print(f\"Micro f1: {mic_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
